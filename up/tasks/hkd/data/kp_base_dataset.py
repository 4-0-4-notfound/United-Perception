from __future__ import division

# Standard Library
import json
import os.path as op

# Import from third library
import numpy as np
import torch
from easydict import EasyDict
from torch.nn.modules.utils import _pair
from torch.utils.data import Dataset

from up.utils.env.dist_helper import get_world_size
from up.utils.general.log_helper import default_logger as logger

# Import from local
from up.data.image_reader import build_image_reader
from up.data.datasets.transforms import build_partially_inverse_transformer, build_transformer
from up.data.metrics.base_evaluator import build_evaluator


class BaseDataset(Dataset):
    """
    A dataset should implement interface below:

    1. :meth:`__len__` to get size of the dataset, Required
    2. :meth:`__getitem__` to get a single data, Required
    3. evaluate to get metrics, Required
    4. dump to output results, Required
    5. visualize gts or dts to check annotations, Optional
    """
    def __init__(self,
                 meta_file,
                 image_reader,
                 transformer,
                 evaluator=None,
                 source='base',
                 class_names=None):
        super(BaseDataset, self).__init__()
        self.meta_file = meta_file
        self.image_reader = build_image_reader(image_reader)
        self.source = source
        self.classes = class_names
        if transformer is not None:
            # add datset handler in transform kwargs in need of mosaic/mixup etc.
            for trans in transformer:
                if 'kwargs' in trans and trans['kwargs'].get('extra_input', False):
                    trans['kwargs']['dataset'] = self
            self.transformer = build_transformer(transformer)
            self.visable_transformer = build_partially_inverse_transformer(self.transformer)
        else:
            self.visable_transformer = self.transformer = lambda x: x

        if evaluator is not None:
            self.evaluator = build_evaluator(evaluator)
        else:
            self.evaluator = None

    def get_image_classes(self, img_index):
        """For Repeat Factor Sampler
        """
        raise NotImplementedError

    @property
    def num_classes(self):
        if hasattr(self, '_num_classes') and self._num_classes is not None:
            return self._num_classes
        assert self.classes is not None
        self._num_classes = len(self.classes)
        return self._num_classes

    @num_classes.setter
    def num_classes(self, num_classes):
        self._num_classes = num_classes

    # @num_classes.setter
    # def num_classes(self, num_classes):
    #     assert hasattr(self, '_num_classes')
    #     self._num_classes = num_classes

    @property
    def num_classes_without_background(self):
        return self.num_classes - 1

    @property
    def class_names(self):
        return self.classes

    @property
    def images_per_class(self):
        """Return a dict with keys of class and values of list of image indices (this idx is called by __iter__)
        """
        raise NotImplementedError

    @property
    def num_images_per_class(self):
        """Return a dict with keys of class and values of number of images of this class
        For Class Aware Balanced Sampler and Repeat Factor Sampler
        """
        raise NotImplementedError

    @property
    def num_instances_per_class(self):
        """Return a dict with keys of class and values of number of instances of this class
        """
        raise NotImplementedError

    def __len__(self):
        """
        Returns dataset length
        """
        raise NotImplementedError

    def __getitem__(self, idx):
        """
        Get a single image data: from dataset

        Arguments:
            - idx (:obj:`int`): index of image, 0 <= idx < len(self)

        """
        raise NotImplementedError

    def _fake_zero_data(self, *size):
        return torch.zeros(size)

    def _generate_grids(self, gt_boxes):
        """
        Generate grid points by (transformed) box coordinates.
        9 points are generated by default.
        :param gt_bboxes: [N, 5]
        :return: grids: [N, num_grid_points(9), 3]
        """
        num_box_kpt = 9
        N = gt_boxes.shape[0]
        grids = np.zeros([N, num_box_kpt, 2])
        x1 = gt_boxes[:, 0]
        y1 = gt_boxes[:, 1]
        x2 = gt_boxes[:, 2]
        y2 = gt_boxes[:, 3]
        grids[:, :3, 0] = np.tile(x1, (3, 1)).transpose()
        grids[:, 3:6, 0] = np.tile((x1 + x2) / 2, (3, 1)).transpose()
        grids[:, 6:, 0] = np.tile(x2, (3, 1)).transpose()
        grids[:, 0::3, 1] = np.tile(y1, (3, 1)).transpose()
        grids[:, 1::3, 1] = np.tile((y1 + y2) / 2, (3, 1)).transpose()
        grids[:, 2::3, 1] = np.tile(y2, (3, 1)).transpose()
        grids = np.hstack([grids.reshape(-1, 2), np.ones([N * num_box_kpt, 1])])
        grids = grids.reshape(N, num_box_kpt, 3)

        return grids

    def evaluate(self, res_file, res=None):
        """
        Arguments:
            - res_file (:obj:`str`): filename
        """
        metrics = self.evaluator.eval(res_file, res) if self.evaluator else {}
        return metrics

    def dump(self, writer, output):
        """
        Dump bboxes with format of (img_id, x1, y1, x2, y2, score class)

        Arguments:
            - writer: output stream
            - output (:obj:`dict`): dict with keys: :code:`{'image_info', 'bboxes', 'filenames'}`

        Output example::

            {
                # (FloatTensor): [B, >=3] (resized_h, resized_w, resize_scale)
                'image_info': <tensor>,
                # (FloatTensor) [N, >=7] (batch_idx, x1, y1, x2, y2, score, cls)
                'bboxes': <tensor>,
                # (list of str): [B], image names
                'filenames': []
            }

        """
        image_ids = output['image_id']
        image_info = self.tensor2numpy(output['image_info'])
        bboxes = self.tensor2numpy(output['dt_bboxes'])
        out_res = []
        for b_ix in range(len(image_info)):
            scale_h, scale_w = _pair(image_info[b_ix][2])
            height, width = image_info[b_ix][3:5]
            img_id = image_ids[b_ix]

            scores = bboxes[:, 5]
            keep_ix = np.where(bboxes[:, 0] == b_ix)[0]
            keep_ix = sorted(keep_ix, key=lambda ix: scores[ix], reverse=True)
            img_bboxes = bboxes[keep_ix].copy()
            img_bboxes[:, 1] /= scale_w
            img_bboxes[:, 2] /= scale_h
            img_bboxes[:, 3] /= scale_w
            img_bboxes[:, 4] /= scale_h

            for bbox in img_bboxes:
                res = {
                    'height': height,
                    'width': width,
                    'image_id': img_id,
                    'bbox': bbox[1: 1 + 4].tolist(),
                    'score': float(bbox[5]),
                    'label': int(bbox[6])
                }
                writer.write(json.dumps(res, ensure_ascii=False) + '\n')
                out_res.append(res)
            writer.flush()
        return out_res

    def merge(self, res_file):
        """
        Merge results into one file

        Arguments:
            - prefix (:obj:`str`): dir/results.rank
        """
        prefix = res_file.rstrip('0123456789')
        world_size = get_world_size()
        merged_file = prefix.rsplit('.', 1)[0] + '.all'
        logger.info(f'concat all results into:{merged_file}')
        merged_fd = open(merged_file, 'w')
        for rank in range(world_size):
            res_file = prefix + str(rank)
            assert op.exists(res_file), f'No such file or directory: {res_file}'
            with open(res_file, 'r') as fin:
                for line_idx, line in enumerate(fin):
                    merged_fd.write(line)
            logger.info(f'merging {res_file} {line_idx+1} results')
        merged_fd.close()
        return merged_file

    def get_np_images(self, images):
        np_images = []
        images = images.cpu()
        for image in images:
            data = EasyDict({'image': image})
            data = self.visable_transformer(data)
            image = data.image.permute(1, 2, 0).contiguous().cpu().float().numpy()
            np_images.append(image)
        np_images = np.stack(np_images, axis=0)
        return np_images.astype(np.uint8)

    def tensor2numpy(self, x):
        if x is None:
            return x
        if torch.is_tensor(x):
            return x.cpu().numpy()
        if isinstance(x, list):
            x = [_.cpu().numpy() if torch.is_tensor(_) else _ for _ in x]
        return x
